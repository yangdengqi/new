# -*- coding: utf-8 -*-
#直接手打
import requests
from bs4 import BeautifulSoup

#pip install jieba
#import jieba.posseg # https://github.com/fxsjy/jieba

def singer_scraping(url):
    articles = []
    r = requests.get(url=URL1)
    soup = BeautifulSoup(r.text, "lxml")
    tag_divs = soup.find_all("div", class_="r-ent")
    for tag in tag_divs:
        if tag.find("a"):
            href = tag.find("a")["href"]
            title = tag.find("a").text
            
            r2=requests.get(url="http://ptt.cc"+href, cookies={"over18":"1"})
            soup2 = BeautifulSoup(r2.text, "lxml")

# New script to add date information of the PTT article
            meta_spans = soup2.find_all("span", class_="article-meta-value")
            
            text = "\n".join(soup2.text.split("\n"))
            articles.append({"title":title, "href":href, "text":text})
    return articles

#pip install jieba
import jieba.posseg # https://github.com/fxsjy/jieba
import time

URL1 = ["https://www.appleofmyeye.com.tw/geshou/dalunan-all-all.htm",
        "https://www.appleofmyeye.com.tw/geshou/dalunv-all-all.htm",
        "https://www.appleofmyeye.com.tw/geshou/daluzuhe-all-all.htm",
        "https://www.appleofmyeye.com.tw/geshou/gangtainan-all-all.htm",
        "https://www.appleofmyeye.com.tw/geshou/gangtainv-all-all.htm",
        "https://www.appleofmyeye.com.tw/geshou/gangtaizuhe-all-all.htm"]

for www in URL1:
    articles = singer_scraping(url=URL)
    
    for article in articles:
        filename = article["href"].split("/")[-1]
       
            
        with open(file="stopword.txt", mode="w", encoding="utf8") as file1:
            tagged_words = jieba.posseg.cut(article["text"])
            file1.write(" ".join(tagged_words))
            print(tagged_words)  
    time.sleep(3)
